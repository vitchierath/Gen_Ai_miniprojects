{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYpCCEg4vxph9iz7/R0lec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitchierath/Gen_Ai_miniprojects/blob/main/multiplepdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK09J4YtXK-6",
        "outputId": "1a136fae-73d3-487f-9a77-bae11e6472e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.55 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.55-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.24 (from langchain-community)\n",
            "  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.22-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.24-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.55-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.52\n",
            "    Uninstalling langchain-core-0.3.52:\n",
            "      Successfully uninstalled langchain-core-0.3.52\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.23\n",
            "    Uninstalling langchain-0.3.23:\n",
            "      Successfully uninstalled langchain-0.3.23\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.24 langchain-community-0.3.22 langchain-core-0.3.55 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import sys\n",
        "import io\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "# Detect if running in Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Check for dependencies\n",
        "try:\n",
        "    if IN_COLAB:\n",
        "        from google.colab import files, drive\n",
        "    else:\n",
        "        import ipywidgets as widgets\n",
        "        from IPython.display import display\n",
        "except ImportError:\n",
        "    print(\"ipywidgets not installed. File upload in Jupyter requires ipywidgets.\")\n",
        "    print(\"Install with: pip install ipywidgets\")\n",
        "    sys.exit(1)\n",
        "\n",
        "required_packages = [\n",
        "    'PyPDF2', 'langchain', 'langchain_huggingface', 'chromadb',\n",
        "    'sentence_transformers', 'transformers', 'torch', 'huggingface_hub'\n",
        "]\n",
        "missing_packages = []\n",
        "for pkg in required_packages:\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except ImportError:\n",
        "        missing_packages.append(pkg)\n",
        "\n",
        "if missing_packages:\n",
        "    print(\"Missing required packages:\", \", \".join(missing_packages))\n",
        "    print(\"Install with:\")\n",
        "    print(\"pip install\", \" \".join(missing_packages))\n",
        "    if IN_COLAB:\n",
        "        print(\"Or in Colab, run: !pip install\", \" \".join(missing_packages))\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Mount Google Drive in Colab.\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        return False\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "def extract_text_from_pdf_file(pdf_file, is_uploaded=False):\n",
        "    \"\"\"Extract text from a single PDF file.\"\"\"\n",
        "    try:\n",
        "        if is_uploaded:\n",
        "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))\n",
        "        else:\n",
        "            with open(pdf_file, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_file if not is_uploaded else 'uploaded file'}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def upload_files_colab():\n",
        "    \"\"\"Handle file uploads in Google Colab.\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        raise EnvironmentError(\"File upload is only supported in Google Colab.\")\n",
        "    print(\"Please upload your PDF files:\")\n",
        "    uploaded = files.upload()\n",
        "    return uploaded\n",
        "\n",
        "def upload_files_jupyter():\n",
        "    \"\"\"Handle file uploads in Jupyter using ipywidgets.\"\"\"\n",
        "    if IN_COLAB:\n",
        "        raise EnvironmentError(\"Jupyter upload not applicable in Colab.\")\n",
        "    uploader = widgets.FileUpload(accept='.pdf', multiple=True)\n",
        "    display(uploader)\n",
        "    print(\"Upload PDFs and then enter 'process' to continue.\")\n",
        "    return uploader\n",
        "\n",
        "def process_uploaded_files(uploaded):\n",
        "    \"\"\"Process uploaded files from Colab or Jupyter.\"\"\"\n",
        "    texts = []\n",
        "    if IN_COLAB:\n",
        "        for filename, file_content in uploaded.items():\n",
        "            if not filename.lower().endswith('.pdf'):\n",
        "                print(f\"Skipping {filename}: Not a PDF file\")\n",
        "                continue\n",
        "            text = extract_text_from_pdf_file(file_content, is_uploaded=True)\n",
        "            if text.strip():\n",
        "                texts.append(text)\n",
        "            else:\n",
        "                print(f\"Warning: No text extracted from {filename}\")\n",
        "    else:\n",
        "        for filename, file_data in uploaded.value.items():\n",
        "            if not filename.lower().endswith('.pdf'):\n",
        "                print(f\"Skipping {filename}: Not a PDF file\")\n",
        "                continue\n",
        "            text = extract_text_from_pdf_file(file_data['content'], is_uploaded=True)\n",
        "            if text.strip():\n",
        "                texts.append(text)\n",
        "            else:\n",
        "                print(f\"Warning: No text extracted from {filename}\")\n",
        "    return texts\n",
        "\n",
        "def extract_text_from_pdfs(source, is_directory=False):\n",
        "    \"\"\"Extract text from PDFs (directory or uploaded files).\"\"\"\n",
        "    texts = []\n",
        "\n",
        "    if is_directory:\n",
        "        pdf_directory = Path(source)\n",
        "        if not pdf_directory.exists():\n",
        "            raise FileNotFoundError(f\"Directory {source} does not exist.\")\n",
        "\n",
        "        for pdf_file in pdf_directory.glob(\"*.pdf\"):\n",
        "            text = extract_text_from_pdf_file(pdf_file, is_uploaded=False)\n",
        "            if text.strip():\n",
        "                texts.append(text)\n",
        "            else:\n",
        "                print(f\"Warning: No text extracted from {pdf_file}\")\n",
        "    else:\n",
        "        texts = process_uploaded_files(source)\n",
        "\n",
        "    return texts\n",
        "\n",
        "def create_vector_store(texts):\n",
        "    \"\"\"Create a vector store from extracted texts.\"\"\"\n",
        "    if not texts:\n",
        "        raise ValueError(\"No valid text extracted from PDFs\")\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "\n",
        "    documents = text_splitter.create_documents(texts)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=f\"/tmp/chroma_{uuid.uuid4()}\")\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "def setup_qa_pipeline():\n",
        "    \"\"\"Set up the question-answering pipeline.\"\"\"\n",
        "    model_name = \"distilbert-base-cased-distilled-squad\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "        qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=-1)\n",
        "        return qa_pipeline\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up QA pipeline: {e}\")\n",
        "        raise\n",
        "\n",
        "def setup_summarization_pipeline():\n",
        "    \"\"\"Set up the summarization pipeline.\"\"\"\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    try:\n",
        "        summarizer = pipeline(\"summarization\", model=model_name, device=-1)\n",
        "        return summarizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up summarization pipeline: {e}\")\n",
        "        raise\n",
        "\n",
        "def generate_summary(texts, summarizer):\n",
        "    \"\"\"Generate a summary of the PDF content.\"\"\"\n",
        "    # Combine all texts into a single string\n",
        "    full_text = \" \".join(texts)\n",
        "    if not full_text.strip():\n",
        "        return \"No valid text found in the PDFs to summarize.\"\n",
        "\n",
        "    # Split into chunks for summarization (BART has token limits)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=100\n",
        "    )\n",
        "    chunks = text_splitter.split_text(full_text)\n",
        "\n",
        "    # Summarize each chunk and combine\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        try:\n",
        "            summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
        "            summaries.append(summary)\n",
        "        except Exception as e:\n",
        "            print(f\"Error summarizing chunk: {e}\")\n",
        "            summaries.append(\"Failed to summarize this chunk.\")\n",
        "\n",
        "    # Combine summaries into a final summary\n",
        "    final_summary = \" \".join(summaries)\n",
        "    if len(final_summary) > 1000:\n",
        "        # Summarize the combined summaries if too long\n",
        "        try:\n",
        "            final_summary = summarizer(final_summary, max_length=300, min_length=100, do_sample=False)[0]['summary_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Error summarizing combined summaries: {e}\")\n",
        "            final_summary = final_summary[:1000] + \"... (truncated)\"\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "def answer_question(question, vectorstore, qa_pipeline):\n",
        "    \"\"\"Answer a question using the vector store and QA pipeline.\"\"\"\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    context = \" \".join([doc.page_content for doc in docs])\n",
        "    if not context.strip():\n",
        "        return \"No relevant content found in the PDFs.\", []\n",
        "\n",
        "    try:\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        answer = result['answer']\n",
        "    except Exception as e:\n",
        "        print(f\"Error in QA pipeline: {e}\")\n",
        "        answer = \"Failed to generate an answer due to pipeline error.\"\n",
        "\n",
        "    return answer, docs\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process PDFs, generate a summary, and answer questions.\"\"\"\n",
        "    if IN_COLAB:\n",
        "        print(\"Choose PDF input method:\")\n",
        "        print(\"1. Upload PDF files\")\n",
        "        print(\"2. Load PDFs from Google Drive directory\")\n",
        "\n",
        "        choice = input(\"Enter 1, 2: \").strip()\n",
        "    else:\n",
        "        print(\"Choose PDF input method:\")\n",
        "        print(\"1. Upload PDF files (Jupyter only)\")\n",
        "        print(\"2. Load PDFs from local directory\")\n",
        "\n",
        "        choice = input(\"Enter 1, 2 \").strip()\n",
        "\n",
        "    texts = []\n",
        "    if choice == \"1\":\n",
        "        if IN_COLAB:\n",
        "            uploaded = upload_files_colab()\n",
        "        else:\n",
        "            try:\n",
        "                uploader = upload_files_jupyter()\n",
        "                user_input = input(\"Enter 'process' to proceed: \").strip().lower()\n",
        "                if user_input != 'process':\n",
        "                    return \"Aborted. Please rerun and enter 'process' after uploading.\", []\n",
        "                uploaded = uploader\n",
        "            except EnvironmentError:\n",
        "                return \"File upload is only supported in Jupyter or Colab environments.\", []\n",
        "        texts = extract_text_from_pdfs(uploaded, is_directory=False)\n",
        "    elif choice == \"2\":\n",
        "        if IN_COLAB:\n",
        "            if mount_google_drive():\n",
        "                pdf_directory = input(\"Enter the Google Drive directory path (e.g., /content/drive/MyDrive/pdfs): \").strip()\n",
        "                try:\n",
        "                    texts = extract_text_from_pdfs(pdf_directory, is_directory=True)\n",
        "                except FileNotFoundError as e:\n",
        "                    return str(e), []\n",
        "            else:\n",
        "                return \"Failed to mount Google Drive.\", []\n",
        "        else:\n",
        "            pdf_directory = input(\"Enter the local directory path (e.g., ./pdfs): \").strip()\n",
        "            try:\n",
        "                texts = extract_text_from_pdfs(pdf_directory, is_directory=True)\n",
        "            except FileNotFoundError as e:\n",
        "                return str(e), []\n",
        "    else:\n",
        "        return \"Invalid choice. Please select 1 or 2.\", []\n",
        "\n",
        "    if not texts:\n",
        "        return \"No valid PDF content found. Please upload PDFs or specify a valid directory.\", []\n",
        "\n",
        "    # Generate summary\n",
        "    print(\"\\nGenerating summary of content...\")\n",
        "    summarizer = setup_summarization_pipeline()\n",
        "    summary = generate_summary(texts, summarizer)\n",
        "    print(\"Summary:\")\n",
        "    print(summary)\n",
        "\n",
        "    # Create vector store for QA\n",
        "    vectorstore = create_vector_store(texts)\n",
        "\n",
        "    # Setup QA pipeline\n",
        "    qa_pipeline = setup_qa_pipeline()\n",
        "\n",
        "    # Chat-like question loop\n",
        "    print(\"\\nAsk follow-up questions (type 'exit' to stop):\")\n",
        "    while True:\n",
        "        question = input(\"You: \").strip()\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Me: Goodbye!\")\n",
        "            break\n",
        "        if not question:\n",
        "            print(\"Me: Please enter a valid question.\")\n",
        "            continue\n",
        "\n",
        "        answer, sources = answer_question(question, vectorstore, qa_pipeline)\n",
        "        print(f\"Me: {answer}\")\n",
        "        print(\"\\nSources:\")\n",
        "        for i, doc in enumerate(sources, 1):\n",
        "            print(f\"{i}. {doc.page_content[:200]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        result = main()\n",
        "        if isinstance(result, tuple):\n",
        "            answer, sources = result\n",
        "            print(f\"Error: {answer}\")\n",
        "            print(\"\\nSources:\")\n",
        "            for i, doc in enumerate(sources, 1):\n",
        "                print(f\"{i}. {doc.page_content[:200]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "9XPruzM7cSel"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqNkBbE7Dwxd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}