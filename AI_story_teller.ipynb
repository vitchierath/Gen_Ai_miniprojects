{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMe/a7Y5CPlbuAKLG1h5tAU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS44banA8GWZ",
        "outputId": "1f3a361d-17b7-4900-8d57-4cfdeba5568e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# # STEP 1: Install dependencies\n",
        "# !pip install transformers --quiet\n",
        "# !pip install accelerate --quiet\n",
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install necessary libraries (if not already)\n",
        "# !pip install transformers gradio torch\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# STEP 3: Load GPT model & tokenizer\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Use GPU if available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# STEP 4: Create generation pipeline\n",
        "story_gen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "# STEP 5: Define story generation function\n",
        "def generate_story(prompt, max_length=400, temperature=0.9, top_p=0.95):\n",
        "    result = story_gen(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# STEP 6: Build Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_story,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Story Prompt\", placeholder=\"Once upon a time ...\"),\n",
        "        gr.Slider(minimum=100, maximum=1000, step=50, value=400, label=\"Max Length\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.5, step=0.1, value=0.9, label=\"Temperature\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.0, step=0.05, value=0.95, label=\"Top-p\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Story\", lines=20),\n",
        "    title=\"📝 GPT-2 Story Generator\",\n",
        "    description=\"Enter a prompt and customize the generation parameters to create your own story using GPT-2 Medium.\"\n",
        ")\n",
        "\n",
        "# STEP 7: Launch the app\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "4OYDB4yI8Ibs",
        "outputId": "8924f560-5981-4eb1-9457-470242166aa0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1d5ba3f6d36caef7de.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1d5ba3f6d36caef7de.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5A68_a8h-Ld0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}