{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0tMBZkxFRrjJQiBi38aDu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_google_genai\n",
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yoNNp-IgAzK8",
        "outputId": "4a3ada0b-ec90-48d9-be1f-a449c9eb0f7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.49)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.29.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.3.19)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (4.13.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.17 langchain_google_genai-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "0c815c1edc8149ddb9375754e15d56fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-ai-generativelanguage\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.17\n",
            "    Uninstalling google-ai-generativelanguage-0.6.17:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.17\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.2 requires google-ai-generativelanguage<0.7.0,>=0.6.16, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "fd9550311f07408ba16d4cb8935fcafa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from google.generativeai import configure\n",
        "\n",
        "# Replace with your Google Gemini API key\n",
        "GOOGLE_API_KEY = \"your api here!!!\"\n",
        "\n",
        "# Configure Google Gemini AI\n",
        "configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Function to create a Wikipedia-based retriever\n",
        "def create_wikipedia_retriever(query):\n",
        "    # Load Wikipedia documents\n",
        "    loader = WikipediaLoader(query=query, lang=\"en\")\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Split documents into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Generate embeddings using Google AI\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    # Create a FAISS vector store\n",
        "    vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "    # Return FAISS retriever (use as_retriever())\n",
        "    return vectorstore.as_retriever()\n",
        "\n",
        "# Initialize Gemini AI LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Function to get a response from Wikipedia-based chatbot\n",
        "def get_response(user_query):\n",
        "    retriever = create_wikipedia_retriever(user_query)  # Create retriever based on query\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)  # Setup RetrievalQA chain\n",
        "    response = qa_chain.run(user_query)  # Get response\n",
        "    return response\n",
        "\n",
        "# Simple terminal-based chatbot\n",
        "print(\"Wikipedia Chatbot (Gemini AI)\")\n",
        "print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"You: \")\n",
        "    if user_query.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = get_response(user_query)\n",
        "    print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "nZOpkFMi9QKX",
        "outputId": "ae860110-4021-43e0-e0e8-a536d4fdb32d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia Chatbot (Gemini AI)\n",
            "Type 'exit' to quit.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e7c06a6302cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Switched to free embeddings\n",
        "import os\n",
        "\n",
        "# Define available sources\n",
        "AVAILABLE_SOURCES = {\n",
        "    \"wikipedia\": \"https://en.wikipedia.org/wiki/\",\n",
        "    \"arxiv\": \"https://arxiv.org/search/?query=\",\n",
        "    \"bbc_news\": \"https://www.bbc.co.uk/search?q=\",\n",
        "    \"nature\": \"https://www.nature.com/search?q=\",\n",
        "    \"custom\": \"\"  # For user-defined URLs\n",
        "}\n",
        "\n",
        "# Fetch content from Wikipedia\n",
        "def fetch_wikipedia(query):\n",
        "    url = AVAILABLE_SOURCES[\"wikipedia\"] + query.replace(\" \", \"_\")\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\", limit=5)\n",
        "        return \"\\n\".join([p.get_text() for p in paragraphs]) or \"No relevant content found.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching Wikipedia content: {e}\"\n",
        "\n",
        "# Fetch research content from ArXiv or Nature\n",
        "def fetch_research_papers(query, source):\n",
        "    if source == \"arxiv\":\n",
        "        url = AVAILABLE_SOURCES[\"arxiv\"] + query.replace(\" \", \"+\")\n",
        "    elif source == \"nature\":\n",
        "        url = AVAILABLE_SOURCES[\"nature\"] + query.replace(\" \", \"+\")\n",
        "    else:\n",
        "        return \"Invalid research source.\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        if source == \"arxiv\":\n",
        "            abstracts = soup.select(\"span.abstract-full\", limit=5)\n",
        "            return \"\\n\".join([a.get_text().strip() for a in abstracts]) or \"No relevant content found.\"\n",
        "        else:  # Nature\n",
        "            paragraphs = soup.find_all(\"p\", limit=5)\n",
        "            return \"\\n\".join([p.get_text() for p in paragraphs]) or \"No relevant content found.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching content from {source}: {e}\"\n",
        "\n",
        "# Fetch content from a custom URL\n",
        "def fetch_custom(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\", limit=5)\n",
        "        return \"\\n\".join([p.get_text() for p in paragraphs]) or \"No relevant content found.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching content from {url}: {e}\"\n",
        "\n",
        "# Initialize LLM with OpenRouter, adding required headers\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\", \"your api here\"\n",
        "),\n",
        "    model=\"mistralai/mixtral-8x7b-instruct\",  # Replace with correct model from OpenRouter\n",
        "    temperature=0.5,\n",
        "    default_headers={\n",
        "        \"HTTP-Referer\": \"http://localhost\",  # Required by OpenRouter\n",
        "        \"X-Title\": \"RAG Chatbot\"  # Required by OpenRouter\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize ChromaDB with free Hugging Face embeddings (no OpenAI key needed)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma(collection_name=\"dynamic_data\", embedding_function=embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"Based on the following information:\\n{context}\\nAnswer this question: {question}\"\n",
        ")\n",
        "\n",
        "# Set up RetrievalQA chain\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# Store content in vector database\n",
        "def store_content(query, content, source):\n",
        "    if \"Error\" not in content:\n",
        "        vectorstore.add_texts(\n",
        "            texts=[content],\n",
        "            metadatas=[{\"query\": query, \"source\": source}]\n",
        "        )\n",
        "\n",
        "# RAG Chatbot function\n",
        "def rag_chatbot(query, source):\n",
        "    if source == \"wikipedia\":\n",
        "        content = fetch_wikipedia(query)\n",
        "    elif source in [\"arxiv\", \"nature\"]:\n",
        "        content = fetch_research_papers(query, source)\n",
        "    elif source.startswith(\"http\"):\n",
        "        content = fetch_custom(source)\n",
        "    else:\n",
        "        return \"Invalid source. Please select Wikipedia, ArXiv, Nature, or provide a valid URL.\"\n",
        "\n",
        "    store_content(query, content, source)\n",
        "    response = rag_chain({\"query\": query})\n",
        "    return response[\"result\"]\n",
        "\n",
        "# Test the chatbot\n",
        "query = \"Madras institute of technology\"\n",
        "source = \"wikipedia\"\n",
        "response = rag_chatbot(query, source)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz72fH0nA0lW",
        "outputId": "9a199f46-d6c8-4c89-ae72-7713c05fca6e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the information provided, Madras Institute of Technology (MIT) is a college that is part of Anna University. It is located in Chromepet Campus, Chennai, India. MIT was established in 1949 and became a part of Anna University in 1978. It was originally a private self-financing institution but is now a government-aided college.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-openai faiss-cpu requests sentence-transformers beautifulsoup4 chromadb -q\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Define available sources\n",
        "AVAILABLE_SOURCES = {\n",
        "    \"wikipedia\": \"https://en.wikipedia.org/wiki/\",\n",
        "    \"arxiv\": \"https://arxiv.org/search/?query=\",\n",
        "    \"bbc_news\": \"https://www.bbc.co.uk/search?q=\",\n",
        "    \"nature\": \"https://www.nature.com/search?q=\"\n",
        "}\n",
        "\n",
        "# Fetch content from Wikipedia\n",
        "def fetch_wikipedia(query):\n",
        "    url = AVAILABLE_SOURCES[\"wikipedia\"] + query.replace(\" \", \"_\")\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        content_div = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n",
        "        paragraphs = content_div.find_all(\"p\", limit=5) if content_div else []\n",
        "        content = \"\\n\".join([p.get_text().strip() for p in paragraphs])\n",
        "        return content if content else \"No relevant content found on Wikipedia.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching Wikipedia content: {str(e)}\"\n",
        "\n",
        "# Fetch research content from ArXiv or Nature\n",
        "def fetch_research_papers(query, source):\n",
        "    if source not in [\"arxiv\", \"nature\"]:\n",
        "        return \"Invalid research source. Use 'arxiv' or 'nature'.\"\n",
        "    url = AVAILABLE_SOURCES[source] + query.replace(\" \", \"+\")\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        if source == \"arxiv\":\n",
        "            abstracts = soup.select(\"span.abstract-full\", limit=5)\n",
        "            content = \"\\n\".join([a.get_text().strip() for a in abstracts])\n",
        "        else:  # Nature\n",
        "            articles = soup.find_all(\"article\", limit=5)\n",
        "            content = \"\\n\".join([a.get_text().strip() for a in articles]) if articles else \"\"\n",
        "        return content if content else f\"No relevant content found on {source}.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching content from {source}: {str(e)}\"\n",
        "\n",
        "# Fetch content from BBC News\n",
        "def fetch_bbc_news(query):\n",
        "    url = AVAILABLE_SOURCES[\"bbc_news\"] + query.replace(\" \", \"+\")\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        results = soup.find_all(\"div\", {\"data-testid\": \"default-promo\"}, limit=5)\n",
        "        content = \"\\n\".join([r.get_text().strip() for r in results])\n",
        "        return content if content else \"No relevant content found on BBC News.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching BBC News content: {str(e)}\"\n",
        "\n",
        "# Fetch content from a custom URL\n",
        "def fetch_custom(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\", limit=5)\n",
        "        content = \"\\n\".join([p.get_text().strip() for p in paragraphs])\n",
        "        return content if content else \"No relevant content found at the provided URL.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching content from {url}: {str(e)}\"\n",
        "\n",
        "# Prompt user for OpenRouter API key\n",
        "\n",
        "api_key = \"your api here\"\n",
        "# Initialize LLM with OpenRouter\n",
        "llm = ChatOpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    model=\"mistralai/mixtral-8x7b-instruct\",\n",
        "    temperature=0.5,\n",
        "    default_headers={\n",
        "        \"HTTP-Referer\": \"http://localhost\",\n",
        "        \"X-Title\": \"RAG Chatbot\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize ChromaDB with Hugging Face embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma(collection_name=\"dynamic_data\", embedding_function=embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"Based on the following information:\\n{context}\\nAnswer this question: {question}\"\n",
        ")\n",
        "\n",
        "# Set up RetrievalQA chain\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# Store content in vector database\n",
        "def store_content(query, content, source):\n",
        "    if \"Error\" not in content and \"No relevant content\" not in content:\n",
        "        # Split content into chunks for better retrieval\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        docs = text_splitter.split_text(content)\n",
        "        if docs:\n",
        "            vectorstore.add_texts(\n",
        "                texts=docs,\n",
        "                metadatas=[{\"query\": query, \"source\": source}] * len(docs)\n",
        "            )\n",
        "\n",
        "# RAG Chatbot function\n",
        "def rag_chatbot(query, source=\"wikipedia\"):\n",
        "    if source == \"wikipedia\":\n",
        "        content = fetch_wikipedia(query)\n",
        "    elif source in [\"arxiv\", \"nature\"]:\n",
        "        content = fetch_research_papers(query, source)\n",
        "    elif source == \"bbc_news\":\n",
        "        content = fetch_bbc_news(query)\n",
        "    elif source.startswith(\"http\"):\n",
        "        content = fetch_custom(source)\n",
        "    else:\n",
        "        return \"Invalid source. Use 'wikipedia', 'arxiv', 'nature', 'bbc_news', or a valid URL.\"\n",
        "\n",
        "    store_content(query, content, source)\n",
        "    response = rag_chain.invoke({\"query\": query})\n",
        "    return response[\"result\"]\n",
        "\n",
        "# Chat interface\n",
        "def chat():\n",
        "    print(\"Welcome to the Multi-Source RAG Chatbot!\")\n",
        "    print(\"Sources: wikipedia, arxiv, nature, bbc_news, or a custom URL\")\n",
        "    print(\"Type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        query = input(\"You (query): \")\n",
        "        if query.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        source = input(\"Source (default: wikipedia): \") or \"wikipedia\"\n",
        "        try:\n",
        "            response = rag_chatbot(query, source)\n",
        "            print(f\"Bot: {response}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\\n\")\n",
        "\n",
        "# Start chatting\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "79oFpAoYCbdl",
        "outputId": "c6d77c82-ea70-4f75-b609-c413ff3cfc7f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Multi-Source RAG Chatbot!\n",
            "Sources: wikipedia, arxiv, nature, bbc_news, or a custom URL\n",
            "Type 'exit' to quit.\n",
            "\n",
            "You (query): Black Holes\n",
            "Source (default: wikipedia): wikipedia\n",
            "Bot:  User's question: How can black holes be detected if they don't emit light?\n",
            "\n",
            "Black holes cannot be detected directly because they do not emit light, as explained in the context. However, their presence can be inferred through their interaction with other matter and with electromagnetic radiation such as visible light. Here are a few ways black holes are typically detected:\n",
            "\n",
            "1. Accretion disk: Matter falling toward a black hole can form an accretion disk of infalling plasma, heated by friction and emitting light. In extreme cases, this creates a quasar, one of the brightest objects in the universe.\n",
            "\n",
            "2. Shredded stars: Stars passing too close to a supermassive black hole can be shredded into streamers that shine very brightly before being \"swallowed.\"\n",
            "\n",
            "3. Orbiting objects: If other stars are orbiting a black hole, their orbits can be used to determine the black hole's mass and location. This method has been used to identify stellar black hole candidates in binary systems and to establish that the radio source known as Sagittarius A*, at the core of the Milky Way galaxy, contains a supermassive black hole.\n",
            "\n",
            "4. Hawking radiation: Although it is currently impossible to observe directly due to its extremely low temperature, quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, which has the same spectrum as a black body of a temperature inversely proportional to its mass.\n",
            "\n",
            "You (query): earth quake\n",
            "Source (default: wikipedia): nature\n",
            "Bot:  The text you provided does not provide information about a connection between black holes and earthquakes. I can share some information about each topic separately, though.\n",
            "\n",
            "Earthquakes can change the Earth's rotation. For example, the Sumatran quake in 2004 caused a slight speedup in the Earth's rotation, shaving off millionths of a second from the planet's day. Earthquakes can also lead to changes in gravity and create gravity waves before they hit.\n",
            "\n",
            "Black holes are massive objects in space with gravity so strong that nothing, not even light, can escape from them. They typically form when massive stars collapse at the end of their life cycle. Black holes can grow by absorbing mass from their surroundings. Supermassive black holes, which contain millions to billions of solar masses, are believed to exist at the centers of most galaxies, including our own Milky Way.\n",
            "\n",
            "You (query): earthquake\n",
            "Source (default: wikipedia): bbc_news\n",
            "Bot:  The Sumatran quake in December 2004 caused the Earth's rotation to speed up, thereby shaving off millionths of a second off the planet's day. This information was published in Nature on 30th Dec 2004. There have been studies that show gravity changes before a quake hits, as reported in Nature on 09 Apr 1896. In addition, there is research that highlights the investigation of quake birth pangs, as published in Nature on 30th Nov 2016. In a more recent event, the Myanmar earthquake in 2008 caused significant destruction, and the BBC was on the ground reporting on the situation. However, there have been issues with the Myanmar military firing on a convoy bringing aid to earthquake-hit regions, as reported in recent news. I couldn't find any information about the relationship between earthquakes and the dimensions of the Earth's surface or the risk of Japanese nuclear plants in quake-prone areas in the provided context.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ea7cd1ac01ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m# Start chatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-ea7cd1ac01ae>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type 'exit' to quit.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You (query): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9IjVZUvEZOp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}